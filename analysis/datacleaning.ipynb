{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Read in SQL Databases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autistic Cohort"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:18<00:00, 32.83it/s]\n"
     ]
    }
   ],
   "source": [
    "conn1 = sqlite3.connect('./Data/knwlg_blf.db')\n",
    "df1 = pd.read_sql(\"SELECT * \"\n",
    "                 \"FROM subjects s \"\n",
    "                 \"JOIN trials t \"\n",
    "                 \"ON s.prolific_id=t.prolific_id \"\n",
    "                 \"JOIN demographics d \"\n",
    "                 \"ON s.prolific_id=d.prolific_id \"\n",
    "                 \"JOIN autism_scores a \"\n",
    "                 \"ON s.prolific_id=a.prolific_id \"\n",
    "                 \"WHERE s.completion_code LIKE '%XXX%' \"\n",
    "                 \"AND s.recaptcha_complete=1 \"\n",
    "                 \"AND t.trial_type='test'\", con=conn1)\n",
    "\n",
    "#df1 = df1.loc[~df1.target_onset.isnull()]\n",
    "#df1 = df1.loc[~df1.AQ_rating_1.isnull()] # this is because 5 ids are in DB multiple times somehow\n",
    "#df1 = df1.loc[~df1.age.isnull()] # this is because 5 ids are in DB multiple times somehow\n",
    "\n",
    "df1 = df1.loc[:,~df1.columns.duplicated()].copy() # prolific id from both tables will be in df, remove duplicate\n",
    "\n",
    "weird = {}\n",
    "for subj in df1.prolific_id.unique():\n",
    "    weird[subj] = len(df1.loc[df1.prolific_id == subj])\n",
    "strange = [key  for (key, value) in weird.items() if value != 12] # these subjects have duplicate rows with no data\n",
    "df1 = df1.loc[~df1.prolific_id.isin(strange)]\n",
    "\n",
    "fel_a = pd.read_sql(\"SELECT * \"\n",
    "                    \"FROM felicities f \"\n",
    "                    \"JOIN subjects s \"\n",
    "                    \"ON s.prolific_id=f.prolific_id \"\n",
    "                    \"WHERE s.completion_code LIKE '%XXX%' \"\n",
    "                    \"AND s.recaptcha_complete=1 \", con=conn1)\n",
    "fel_a = fel_a.loc[:,~fel_a.columns.duplicated()].copy() # prolific id from both tables will be in df, remove duplicate\n",
    "fel_a = fel_a.reset_index()\n",
    "\n",
    "# Add Felicity rating to appropriate row\n",
    "df1['felicity_rating'] = 999\n",
    "for pp in tqdm(df1.prolific_id.unique()):\n",
    "    for i, trl in df1.loc[df1.prolific_id == pp].iterrows():\n",
    "        df1.loc[(df1.prolific_id == pp) & (df1.trial_num == trl.trial_num), 'felicity_rating'] = fel_a.loc[(fel_a.prolific_id == pp) & (fel_a.block1_trial_num == trl.trial_num)].felicity_rating.values[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correcting Responses and RT in autism group"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def convert_time(x):\n",
    "    if type(x) == str:\n",
    "        return datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "df1[['response_onset','target_onset','participation_date','completion_time']] = df1[['response_onset','target_onset','participation_date','completion_time']].applymap(convert_time)\n",
    "\n",
    "df1['rt'] = df1.apply(lambda row: row['response_onset'] - row['target_onset'], axis=1)\n",
    "df1['rt_ms'] = df1.apply(lambda row: int(round(row['rt'].total_seconds() * 1e3)), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df1['response_key'] = df1.response_key.apply(eval)\n",
    "df1['keypress_time'] = [eval(x) for x in df1.keypress_time]\n",
    "df1 = df1[[xx == list for xx in [type(x) for x in df1.response_key]]].reset_index(drop=True) # make sure responses are saved as a list.\n",
    "# If subjects hit more than one key, use only the last one\n",
    "df1['full_response'] = df1['response_key']\n",
    "df1['response_key'] = [resp[-1].lower() for resp in df1.response_key]\n",
    "df1['corrected_resp'] = df1.response_key\n",
    "df1['corrected_rt'] = df1.rt_ms\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for i, trl in df1[df1.apply(lambda row: len(row['full_response']) > 1, axis=1)].iterrows(): # iterate through rows where >1  response was made\n",
    "    keys_pressed = list(Counter(trl.full_response).keys()) # list the unique keys, in the temporal order they were pressed. fjf -> fj\n",
    "    if '999' in keys_pressed: # if the timer hit 5 seconds\n",
    "        if keys_pressed.index('999') == 0: # and there were no other keys pressed before then\n",
    "            intended_answer = '999' # their answer is recorded as 999\n",
    "        else : # if there were other keys pressed before the 5-second timer (but the 5 seconds  was still reached because of internet transfer)\n",
    "            intended_answer = keys_pressed[:keys_pressed.index('999')][-1] # the last key pressed before the 999 was the intended response\n",
    "    else:     # If there is no timeout, take the first instance of the last key pressed.\n",
    "        kp = [x for x in keys_pressed if x =='f'or x== 'j']\n",
    "        if len(kp) > 0:\n",
    "            intended_answer = kp[-1]\n",
    "        else:\n",
    "            intended_answer = keys_pressed[-1]\n",
    "\n",
    "    onset = trl.keypress_time[trl.full_response.index(intended_answer)] #onset of the first instance of the last key pressed.\n",
    "    rt = int((onset - trl.target_onset).total_seconds() * 1e3)  # compute the RT for that key press\n",
    "    df1.loc[df1.id == trl.id, ['response_key', 'rt_ms', 'response_onset']] = intended_answer, rt, onset\n",
    "\n",
    "# SUSPICIOUS EMAILS\n",
    "df1 = df1[[\"mailfence\" not in x for x in df1.email]].reset_index(drop=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neurotypical Cohort"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:13<00:00, 37.39it/s]\n"
     ]
    }
   ],
   "source": [
    "conn2 = sqlite3.connect('./Data/knwlg_blf_controls.db')\n",
    "df2 = pd.read_sql(\"SELECT * \"\n",
    "                 \"FROM subjects s \"\n",
    "                 \"JOIN trials t \"\n",
    "                 \"ON s.prolific_id=t.prolific_id \"\n",
    "                 \"JOIN demographics d \"\n",
    "                 \"ON s.prolific_id=d.prolific_id \"\n",
    "                 \"JOIN autism_scores a \"\n",
    "                 \"ON s.prolific_id=a.prolific_id \"\n",
    "                 \"WHERE s.completion_code LIKE '%548DA3BD%' \"\n",
    "                 \"AND t.trial_type='test'\", con=conn2)\n",
    "\n",
    "df2 = df2.loc[~df2.target_onset.isnull()]\n",
    "df2 = df2.loc[~df2.AQ_rating_1.isnull()] # this is because 5 ids are in DB multiple times somehow\n",
    "\n",
    "df2 = df2.loc[:,~df2.columns.duplicated()].copy() # prolific id from both tables will be in df, remove duplicate\n",
    "\n",
    "fel_b = pd.read_sql(\"SELECT * \"\n",
    "                    \"FROM felicities f \"\n",
    "                    \"JOIN subjects s \"\n",
    "                    \"ON s.prolific_id=f.prolific_id \"\n",
    "                    \"WHERE s.completion_code LIKE '%548DA3BD%' \", con=conn2)\n",
    "fel_b = fel_b.loc[:,~fel_b.columns.duplicated()].copy() # prolific id from both tables will be in df, remove duplicate\n",
    "fel_b = fel_b.reset_index()\n",
    "\n",
    "# Add Felicity rating to appropriate row\n",
    "df2['felicity_rating'] = 999\n",
    "for pp in tqdm(df2.prolific_id.unique()):\n",
    "    for i, trl in df2.loc[df2.prolific_id == pp].iterrows():\n",
    "        df2.loc[(df2.prolific_id == pp) & (df2.trial_num == trl.trial_num), 'felicity_rating'] = fel_b.loc[(fel_b.prolific_id == pp) & (fel_b.block1_trial_num == trl.trial_num)].felicity_rating.values[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df2['response_key'] = df2.response_key.apply(eval)\n",
    "df2['response_key'] = df2.apply(lambda row: row['response_key'][-1].lower(), axis=1)\n",
    "df2[['response_onset','target_onset','participation_date']] = df2[['response_onset','target_onset','participation_date']].applymap(convert_time)\n",
    "\n",
    "df2['rt'] = df2.apply(lambda row: row['response_onset'] - row['target_onset'], axis=1)\n",
    "df2['rt_ms'] = df2.apply(lambda row: int(round(row['rt'].total_seconds() * 1e3)), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting Columns to match"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df1['group'] = 'autism'\n",
    "df2['group'] = 'neurotypical'\n",
    "df2['diag'] = 'n/a'\n",
    "COI = ['prolific_id', 'participation_date', 'GMT_timestamp', 'trial_num', 'correct', 'scenario', 'belief_type', 'ascription_type','correct_answer', 'rt_ms','response_key', 'age', 'gender', 'ethnicity', 'education', 'AQ_rating_1', 'AQ_rating_2', 'AQ_rating_3', 'AQ_rating_4', 'AQ_rating_5', 'AQ_rating_6', 'AQ_rating_7','AQ_rating_8', 'AQ_rating_9', 'AQ_rating_10', 'felicity_rating', 'group', 'diag']\n",
    "df1 = df1[sorted(COI)]\n",
    "\n",
    "df2 = df2[sorted(COI)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combining Cohorts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# columns in autism group that are not in controls\n",
    "if len(df1.columns[[x not in df2.columns for x in df1.columns]].tolist())== 0:\n",
    "    dat = pd.concat([df1, df2], axis=0)\n",
    "if (len(df1.prolific_id.unique()) + len(df2.prolific_id.unique())) == len(dat.prolific_id.unique()):\n",
    "\n",
    "    dat['Infelicity Rating'] = 8 - dat.felicity_rating\n",
    "    dat.rename(columns={'rt_ms': 'rt', 'ascription_type': 'ascription', 'belief_type': 'agent_state', 'Infelicity Rating': 'infelicity', 'prolific_id':'subject', 'scenario':'item'}, inplace=True)\n",
    "\n",
    "    for old, new in {'IG':'Ignorance', 'TB':'True Info', 'FB':'False Info'}.items():\n",
    "        dat.agent_state.replace(old, new, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exclusions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Timeouts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dat['timeout'] = False\n",
    "dat.loc[dat.response_key == '999', 'timeout'] = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute Accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1058/1058 [00:02<00:00, 393.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#  correct answers\n",
    "dat.loc[dat.agent_state == 'True Info', 'correct_answer'] = 'j'\n",
    "dat.loc[dat.agent_state != 'True Info', 'correct_answer'] = 'f'\n",
    "# update correct column based on new answer key\n",
    "dat['correct'] = dat.apply(lambda row: row['response_key'] == row['correct_answer'], axis=1)\n",
    "\n",
    "dat.loc[dat.timeout == True, 'correct'] = False # timeouts are considered incorrect\n",
    "\n",
    "threshold = .67\n",
    "dat[['accurate', 'too_fast', 'too_slow', 'in_sample']] = False\n",
    "\n",
    "for subj in tqdm(dat.subject.unique()):\n",
    "    sdat = dat.loc[dat.subject == subj]\n",
    "    if len(sdat) != 12:\n",
    "        print('ohshit')\n",
    "        break\n",
    "    if sdat.correct.mean() >= threshold:\n",
    "        dat.loc[dat.subject == subj, 'accurate'] = True\n",
    "    if sdat.rt.mean() < 1000:\n",
    "        dat.loc[dat.subject == subj, 'too_fast'] = True\n",
    "    if sdat.rt.mean() > 4000:\n",
    "        dat.loc[dat.subject == subj, 'too_slow'] = True\n",
    "\n",
    "\n",
    "dat.loc[(dat.accurate == True) & (dat.too_fast == False) & (dat.too_slow == False), 'in_sample'] = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trial Exclusions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "dat.loc[(dat.rt < 1000) | (dat.rt > 4500), 'in_sample'] = False\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autism Quotient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "## Scoring\n",
    "agree = ['AQ_rating_1', 'AQ_rating_7', 'AQ_rating_8', 'AQ_rating_10'] # score of 3 or 4 get a point\n",
    "disagree = ['AQ_rating_2','AQ_rating_3','AQ_rating_4','AQ_rating_5','AQ_rating_6','AQ_rating_9', ] # score of 1 or 2 get a point]\n",
    "a = dat[agree] >= 3\n",
    "b = dat[disagree] < 3\n",
    "dat['AQ_score'] = a.sum(axis=1) + b.sum(axis=1)\n",
    "dat.drop(['AQ_rating_'+str(i) for i in range(1, 11)], inplace=True, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save the clean file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "dat.to_csv('./Data/full_sample_clean.csv', sep=',', header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}